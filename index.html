<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Aalborg Intelligence">

<title>Perceptroner</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="perceptron-note_files/libs/clipboard/clipboard.min.js"></script>
<script src="perceptron-note_files/libs/quarto-html/quarto.js"></script>
<script src="perceptron-note_files/libs/quarto-html/popper.min.js"></script>
<script src="perceptron-note_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="perceptron-note_files/libs/quarto-html/anchor.min.js"></script>
<link href="perceptron-note_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="perceptron-note_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="perceptron-note_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="perceptron-note_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="perceptron-note_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indhold</h2>
   
  <ul>
  <li><a href="#rosenblatts-perceptron" id="toc-rosenblatts-perceptron" class="nav-link active" data-scroll-target="#rosenblatts-perceptron">Rosenblatts perceptron</a>
  <ul class="collapse">
  <li><a href="#eksempel-hvem-skal-jeg-stemme-på-ved-næste-valg" id="toc-eksempel-hvem-skal-jeg-stemme-på-ved-næste-valg" class="nav-link" data-scroll-target="#eksempel-hvem-skal-jeg-stemme-på-ved-næste-valg">Eksempel: Hvem skal jeg stemme på ved næste valg?</a></li>
  <li><a href="#video-hvad-er-en-perceptron" id="toc-video-hvad-er-en-perceptron" class="nav-link" data-scroll-target="#video-hvad-er-en-perceptron">VIDEO: Hvad er en perceptron?</a></li>
  </ul></li>
  <li><a href="#perceptron-learning-algoritmen" id="toc-perceptron-learning-algoritmen" class="nav-link" data-scroll-target="#perceptron-learning-algoritmen">Perceptron Learning Algoritmen</a>
  <ul class="collapse">
  <li><a href="#video-perceptron-learning-algoritmen" id="toc-video-perceptron-learning-algoritmen" class="nav-link" data-scroll-target="#video-perceptron-learning-algoritmen">VIDEO: Perceptron Learning Algoritmen</a></li>
  </ul></li>
  <li><a href="#det-kan-gøres-smartere-med-adaline" id="toc-det-kan-gøres-smartere-med-adaline" class="nav-link" data-scroll-target="#det-kan-gøres-smartere-med-adaline">Det kan gøres smartere med “Adaline”</a>
  <ul class="collapse">
  <li><a href="#video-perceptron-learning-versus-adaline" id="toc-video-perceptron-learning-versus-adaline" class="nav-link" data-scroll-target="#video-perceptron-learning-versus-adaline">VIDEO: Perceptron Learning versus Adaline</a></li>
  <li><a href="#gradientnedstigning" id="toc-gradientnedstigning" class="nav-link" data-scroll-target="#gradientnedstigning">Gradientnedstigning</a></li>
  <li><a href="#eksempel-på-gradientnedstigning" id="toc-eksempel-på-gradientnedstigning" class="nav-link" data-scroll-target="#eksempel-på-gradientnedstigning">Eksempel på gradientnedstigning</a></li>
  <li><a href="#forskel-på-perceptron-learning-og-gradientnedstigning" id="toc-forskel-på-perceptron-learning-og-gradientnedstigning" class="nav-link" data-scroll-target="#forskel-på-perceptron-learning-og-gradientnedstigning">Forskel på perceptron learning og gradientnedstigning</a></li>
  <li><a href="#video-adaline" id="toc-video-adaline" class="nav-link" data-scroll-target="#video-adaline">VIDEO: Adaline</a></li>
  </ul></li>
  <li><a href="#noten-som-pdf" id="toc-noten-som-pdf" class="nav-link" data-scroll-target="#noten-som-pdf">Noten som pdf</a></li>
  <li><a href="#videre-læsning" id="toc-videre-læsning" class="nav-link" data-scroll-target="#videre-læsning">Videre læsning</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Perceptroner</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Forfatter</div>
    <div class="quarto-title-meta-contents">
             <p>Aalborg Intelligence </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<section id="rosenblatts-perceptron" class="level1 page-columns page-full">
<h1>Rosenblatts perceptron</h1>
<p>Perceptroner... Hvorfor nu det? Jo, for det er faktisk sådan nogle, du ikke vidste, at du ikke kunne leve uden! Nu skal du høre hvorfor.</p>
<section id="eksempel-hvem-skal-jeg-stemme-på-ved-næste-valg" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="eksempel-hvem-skal-jeg-stemme-på-ved-næste-valg">Eksempel: Hvem skal jeg stemme på ved næste valg?</h2>
<p>De senere år er det blevet populært, at diverse medier laver forskellige kandidattests. Sådan nogle tests kan laves på mange forskellige måder - man kunne blandt andet bruge perceptroner! Testene fungerer som regel på den måde, at man bliver stillet en række forskellige spørgsmål og så skal man svare på en skala fra <em>meget uenig</em> til <em>meget enig</em>. Disse kategorier af svar kunne f.eks. oversættes til matematik på denne måde:</p>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped">
<colgroup>
<col style="width: 15%">
<col style="width: 23%">
<col style="width: 20%">
<col style="width: 25%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Helt enig</th>
<th style="text-align: center;">Overvejende enig</th>
<th style="text-align: center;">Hverken/eller</th>
<th style="text-align: center;">Overvejende uenig</th>
<th style="text-align: center;">Helt uenig</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">-2</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Lad os prøve at gøre det helt simpelt. I stedet for at komme med et bud på hvem man skal stemme på, så vil vi blot forsøge at komme med et bud på, om man skal stemme på rød eller blå blok (det er sikkert en håbløs simplificering, men det må du tale med din samfundsfagslærer om ).</p>
<p>Lad os sige at vi vil basere vores bud på to spørgsmål:</p>
<ol type="1">
<li><p>Jeg synes, at indkomstskatten skal sættes ned.</p></li>
<li><p>Jeg synes ikke, at danske virksomheder skal pålægges en CO2-afgift.</p></li>
</ol>
<p>Vi kan sikkert hurtigt blive enige om, at hvis man er meget enig i begge spørgsmål, så hører man formentlig til i blå blok og modsat, hvis man er meget uenig i begge spørgsmål, så hører man nok mere hjemme i rød blok. Så at lave en perceptron, som kan hjælpe os med at forudsige det, er nok ikke raketvidenskab, men det kan ikke desto mindre hjælpe os med at forstå de bagvedliggende principper og hvordan disse sidenhen kan generaliseres.</p>
<p>Lad os prøve at blive lidt mere specifikke og indføre to variable <span class="math inline">\(x_1\)</span> og <span class="math inline">\(x_2\)</span>, hvor</p>
<ul>
<li><span class="math inline">\(x_1\)</span>: svaret på <em>Jeg synes, at indkomstskatten skal sættes ned</em> angivet på en skala fra -2 til 2<br>
</li>
<li><span class="math inline">\(x_2\)</span>: svaret på <em>Jeg synes ikke, at danske virksomheder skal pålægges en CO2-afgift</em> angivet på en skala fra -2 til 2.</li>
</ul>
<p>Vores beslutning vil vi nu også kvantificere vha. en variabel <span class="math inline">\(t\)</span>, som kan antage to værdier, nemlig <span class="math inline">\(-1\)</span> og <span class="math inline">\(1\)</span>. Hvis vi hører hjemme i blå blok, vil vi sætte <span class="math inline">\(t=1\)</span>, mens vi vil sætte <span class="math inline">\(t=-1\)</span>, hvis vi vil sætte vores krydset ved et rødt parti. Altså:</p>
<p><span class="math display">\[
\begin{aligned}
t&amp;=-1: &amp;\text{Rød blok} \\
t&amp;=1: &amp;\text{Blå blok} \\
\end{aligned}
\]</span></p>
<p>Nu forestiller vi os, at vi har bedt seks personer (som godt ved, hvem de vil stemme på - måske er det ligefrem politikere vi har spurgt) om at svare på de to spørgsmål og samtidig tilkendegive, om de vil stemme på blå eller rød blok. Lad os f.eks. sige, at den første person er meget enig i at indkomstskatten skal sættes ned (dvs. <span class="math inline">\(x_1=2\)</span>), og at denne person er overvejende enig i at danske virksomheder ikke skal pålægges en CO2-afgift (dvs. <span class="math inline">\(x_2=1\)</span>). Desuden oplyser denne person, at han/hun vil stemme på blå blok (dvs. <span class="math inline">\(t=1\)</span>). Det kan udtrykkes sådan her: <span id="eq-trainingdata"><span class="math display">\[
(x_1,x_2)=(2,1) \quad \Rightarrow \quad  t=1
\tag{1}\]</span></span></p>
<p>Og sådan kunne man opstille andre eksempler: <span class="math display">\[
\begin{aligned}
&amp;(x_1,x_2)=(-1,1) \quad \Rightarrow \quad  t=-1 \\
&amp;(x_1,x_2)=(-1,-1) \quad \Rightarrow \quad  t=-1 \\
&amp;(x_1,x_2)=(1,1) \quad \Rightarrow \quad  t=1 \\
&amp;(x_1,x_2)=(2,2) \quad \Rightarrow \quad  t=1 \\
&amp;(x_1,x_2)=(-2,-1) \quad \Rightarrow \quad t=-1 \\
\end{aligned}
\]</span> Det første eksempel siger for eksempel, at en person har været overvejende uenig i at sætte indkomstskatten ned (<span class="math inline">\(x_1=-1\)</span>), overvejende enig i at danske virksomheder ikke skal pålægges en CO2-afgift (<span class="math inline">\(x_2=1\)</span>) og samtidig vil denne person stemme på rød blok (<span class="math inline">\(t=-1\)</span>).</p>
<p>Vi kan prøve at indtegne <span class="math inline">\((x_1,x_2)\)</span>-punkterne i et koordinatsystem og samtidig angive den tilhørende værdi af <span class="math inline">\(t\)</span> med en farve. Det vil se sådan her ud:</p>
<div class="cell page-columns page-full">
<div class="cell-output-display page-columns page-full">
<div id="fig-punkter_kandidattest" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="perceptron-note_files/figure-html/fig-punkter_kandidattest-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption margin-caption">Figur&nbsp;1: Illustration af svaret på spørgsmål 1 (<span class="math inline">\(1.\)</span> aksen) og spørgsmål 2 (<span class="math inline">\(2.\)</span> aksen) med en markering af om man vil stemme på rød eller blå blok.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Det kunne godt se ud som om, at det vil være muligt at indtegne en ret linje på en sådan måde, at alle punkter som ligger over linjen skulle farves blå (svarende til "her stemmer vi på blå blok"), mens alle punkter under linjen skulle farves røde (svarende til "her stemmer vi på rød blok"). En tilfældig indtegnet linje ses på <a href="#fig-punkter_kandidattest_graf1">figur&nbsp;2</a>.</p>
<div class="cell page-columns page-full">
<div class="cell-output-display page-columns page-full">
<div id="fig-punkter_kandidattest_graf1" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="perceptron-note_files/figure-html/fig-punkter_kandidattest_graf1-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption margin-caption">Figur&nbsp;2: Illustration af svaret på spørgsmål 1 (<span class="math inline">\(1.\)</span> aksen) og spørgsmål 2 (<span class="math inline">\(2.\)</span> aksen) med en markering af om man vil stemme på rød eller blå blok. En tilfældig linje er indtegnet.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Herunder ser du et bud på en linje, som ser ud til at være god til at adskille de blå punkter fra de røde – faktisk er der jo uendeligt mange linjer, som vil kunne adskille de blå punkter fra de røde:</p>
<div class="cell page-columns page-full">
<div class="cell-output-display page-columns page-full">
<div id="fig-punkter_kandidattest_graf2" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="perceptron-note_files/figure-html/fig-punkter_kandidattest_graf2-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption margin-caption">Figur&nbsp;3: Illustration af svaret på spørgsmål 1 (<span class="math inline">\(1.\)</span> aksen) og spørgsmål 2 (<span class="math inline">\(2.\)</span> aksen) med en markering af om man vil stemme på rød eller blå blok. Her er indtegnet en linje, som kan separere de blå punkter fra de røde.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Linjen på <a href="#fig-punkter_kandidattest_graf2">figur&nbsp;3</a> har ligning <span class="math display">\[\begin{aligned}
y=-1.2 \cdot x+1.5.\end{aligned}\]</span> Men nu kaldte vi jo faktisk ikke de to variable for <span class="math inline">\(x\)</span> og <span class="math inline">\(y\)</span>, men derimod for <span class="math inline">\(x_1\)</span> og <span class="math inline">\(x_2\)</span>. Med denne notation får vi altså, at <span class="math display">\[
\begin{aligned}
x_2=-1.2 \cdot x_1+1.5
\end{aligned}
\]</span> Hvis vi bruger denne ligning til at skelne imellem blå og røde punkter, så vil vi sige, at alle punkter, som ligger over linjen skal være blå. Det vil være det samme som at sige, at alle de blå punkter opfylder uligheden <span class="math display">\[
\begin{aligned}
x_2&gt;-1.2 \cdot x_1+1.5.
\end{aligned}
\]</span> Eller skrevet på en anden måde: <span class="math display">\[
\begin{aligned}
1.2 \cdot x_1+ 1 \cdot x_2&gt;1.5.
\end{aligned}
\]</span> Her kalder man værdi <span class="math inline">\(1.5\)</span> på højreside for <em>threshold</em> værdien (på dansk: tærskelværdi), fordi det er denne værdi, som afgør, om vi skal farve et punkt rødt eller blåt. Værdierne <span class="math inline">\(1.2\)</span> og <span class="math inline">\(1\)</span> kaldes for vægte, fordi de bestemmer, hvor meget inputværdierne <span class="math inline">\(x_1\)</span> og <span class="math inline">\(x_2\)</span> skal vægtes i forhold til hinanden.</p>
<div class="page-columns page-full"><p>En helt tredje måde at skrive det samme på vil være <span class="math display">\[
\begin{aligned}
-1.5+1.2 \cdot x_1+ 1 \cdot x_2&gt;0.
\end{aligned}
\]</span> Nu kalder man så bare værdien <span class="math inline">\(-1.5\)</span> for en bias, men i virkeligheden er det jo bare threshold værdien med modsat fortegn<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;Der er forskellige overvejelser i forhold til valget af denne skrivemåde. For det første er vi gået væk fra <span class="math inline">\(x\)</span> og <span class="math inline">\(y\)</span> og over til <span class="math inline">\(x_1\)</span> og <span class="math inline">\(x_2\)</span>. Det giver mening, fordi vi ofte tænker på <span class="math inline">\(y\)</span> som den afhængige variabel og <span class="math inline">\(x\)</span> som den uafhængige variabel. Denne fortolkning af de to variable giver ikke mening i denne sammenhæng. Derudover kan vi beskrive en vilkårlig linje i planen ved hjælp af ligningen <span class="math inline">\(ax_1+bx_2+c=0\)</span> – også de lodrette linjer. Holder vi derimod fast i <span class="math inline">\(y=ax+b\)</span>, så kan vi ikke “fange” de lodrette linjer.</p></li></div></div>
<p>Vi har nu faktisk udledt en regel, som for tid og evighed kan hjælpe os med at afgøre, om vi skal stemme på rød eller blå blok. Den kan opsummeres sådan her:</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Hvem skal jeg stemme på?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Svar på en skala fra -2 til 2 på følgende spørgsmål:</p>
<p><span class="math inline">\(x_1\)</span>: "Jeg synes, at indkomstskatten skal sættes ned"</p>
<p><span class="math inline">\(x_2\)</span>: "Jeg synes ikke, at danske virksomheder skal pålægges en CO2-afgift"</p>
<p>hvor 2 svarer til "Meget enig" og -2 svarer til "Meget uenig".</p>
<p>Beregn nu <span class="math inline">\(o\)</span> (for outputværdi) på denne måde <span class="math display">\[\begin{aligned}
o = \begin{cases}
1 &amp; \text{hvis } -1.5+1.2 \cdot x_1+ 1 \cdot x_2 \geq 0 \\
-1 &amp; \text{hvis } -1.5+1.2 \cdot x_1+ 1 \cdot x_2 &lt; 0. \\
\end{cases}\end{aligned}\]</span> Reglen er nu: <span class="math display">\[\begin{aligned}
&amp;\text{Hvis } o=1: \quad &amp;\text{Stem blå blok.}\\
&amp;\text{Hvis } o=-1: \quad &amp;\text{Stem rød blok.}\\\end{aligned}\]</span></p>
</div>
</div>
<p>Man siger også, at man på baggrund af inputværdierne kan lave en klassificering (eller kategorisering). Det betyder, at vi på baggrund af inputværdierne kan beregne, om vi er i kategorien "Blå blok" (<span class="math inline">\(o=1\)</span>) eller i kategorien "Rød blok" (<span class="math inline">\(o=-1\)</span>). Grafisk svarer det til, at man indtegner sit <span class="math inline">\((x_1, x_2)\)</span>-punkt i koordinatsystemet i <a href="#fig-punkter_kandidattest_graf2">figur&nbsp;3</a> og ser så på om punkt ligger over eller under linjen (ligger det over skal vi stemme blå blok).</p>
<div id="exm-Rosenblatts" class="theorem example">
<p><span class="theorem-title"><strong>Eksempel 1 </strong></span><em>Lad os sige at en vælger hverken er enig eller uenig i, at indkomstskatten skal sættes ned. Det vil sige, at <span class="math inline">\(x_1=0\)</span>. Samtidig er denne vælger meget enig i, at danske virksomheder ikke skal pålægges en CO2-afgift. Altså er <span class="math inline">\(x_2=2\)</span>. Vi udregner nu: <span class="math display">\[
-1.5+1.2 \cdot x_1+x_2=-1.5+1.2 \cdot 0+2=0.5
\]</span> Og da denne værdi er større end <span class="math inline">\(0\)</span>, sætter vi <span class="math inline">\(o=1\)</span>. Det vil sige, at vi vil anbefale denne vælger at stemme blå blok.</em></p>
</div>
<p>Det er da smart! Og det her er faktisk lige præcis idéen bag perceptroner, som den amerikanske psykolog Frank Rosenblatt foreslog helt tilbage i <span class="math inline">\(1958\)</span>. Den klassiske perceptron er defineret ved, at perceptronen kan modtage input <span class="math display">\[
\begin{aligned}
x_1, x_2, \dots, x_n,
\end{aligned}
\]</span> hvor hver enkel inputværdi i princippet kan være et vilkårligt reelt tal. I vores eksempel har vi dog begrænset inputværdierne til <span class="math inline">\(x_1, x_2 \in \{-2,-1,0,1,2 \}\)</span>. Vi beregner så en outputværdi <span class="math inline">\(o\)</span> vha. vægtene <span class="math inline">\(w_1, w_2, \dots, w_n\)</span> og en biasværdi, som vi her vil kalde for <span class="math inline">\(w_0\)</span> på denne måde: <span class="math display">\[
\begin{aligned}
o = \begin{cases}
1 &amp; \text{hvis } w_0 + w_1 \cdot x_1 + \cdots + w_n \cdot x_n \geq 0 \\
-1 &amp; \text{hvis } w_0 + w_1 \cdot x_1 + \cdots + w_n \cdot x_n &lt; 0. \\
\end{cases}
\end{aligned}
\]</span> Grafisk kan det illustreres sådan her:</p>
<div id="fig-perceptron" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/perceptron.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption margin-caption">Figur&nbsp;4: Grafisk illustration af en perceptron.</figcaption><p></p>
</figure>
</div>
<p>Her illustrerer sumtegnet i cirklen, at vi tager en vægtet sum af alle inputværdierne (inklusiv et input (<span class="math inline">\(x_0\)</span>), som altid er <span class="math inline">\(1\)</span>, og som vægtes med <span class="math inline">\(w_0\)</span> svarende til, at vi får vores bias med), mens grafen af trappefunktionen i firkanten viser, at vi diskretiserer denne vægtede sum, sådan at outputværdien enten er <span class="math inline">\(-1\)</span> eller <span class="math inline">\(1\)</span>.</p>
</section>
<section id="video-hvad-er-en-perceptron" class="level2">
<h2 class="anchored" data-anchor-id="video-hvad-er-en-perceptron">VIDEO: Hvad er en perceptron?</h2>
<p>I denne video forklarer vi ovenstående, men med udgangspunkt i et andet eksempel.</p>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/1WTZZCx-pRY" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
</section>
<section id="perceptron-learning-algoritmen" class="level1 page-columns page-full">
<h1>Perceptron Learning Algoritmen</h1>
<p>Det er jo alt sammen meget fint, hvis man kender vægtene. Bum – så kan man beregne den vægtede sum <span class="math display">\[
\begin{aligned}
w_0 + w_1 \cdot x_1 + \cdots + w_n \cdot x_n.
\end{aligned}
\]</span> Får vi et ikke-negativt tal sættes <span class="math inline">\(o\)</span> til <span class="math inline">\(1\)</span> og <span class="math inline">\(-1\)</span> ellers. Præcis som vi gjorde det i <a href="#exm-Rosenblatts">eksempel&nbsp;1</a>. Men det kræver jo, at man kender vægtene <span class="math inline">\(w_0, w_1, \dots, w_n\)</span>...! I vores eksempel med bare to inputværdier var det nemt nok at finde nogle passende værdier af vægtene. Vi tegnede bare punkterne <span class="math inline">\((x_1,x_2)\)</span> ind i et koordinatsystem og slog en streg, der adskilte de røde punkter fra de blå punkter. Men hvis der er mere end to inputværdier (hvis man f.eks. i kandidattesten skal svare på 20 spørgsmål), så er det jo ikke helt så nemt! Hvad gør vi så?</p>
<p>Det havde Frank Rosenblatt faktisk også en idé til, som vi nu skal se nærmere på. Vi skal forestille os, at vi har en masse data, som vi så det i vores eksempel. Disse data kalder vi for træningsdata, og de vil dels bestå af konkrete inputværdier <span class="math inline">\(x_1, x_2, \dots, x_n\)</span> og den tilhørende korrekte klassificering <span class="math inline">\(t\)</span>. Her bruger vi bogstavet <span class="math inline">\(t\)</span> som en forkortelse for <em>target-værdi</em> – altså den "målværdi", som svarer til den rigtige klassificering. I vores eksempel viser (<a href="#eq-trainingdata">1</a>) et eksempel på sådan et træningsdatasæt.</p>
<p>Lad os så se på hvad Rosenblatts "Perceptron Learning Algoritme" går nu ud på. Den kommer her:</p>
<ul>
<li><p>Sæt alle vægte <span class="math inline">\(w_0, w_1, \dots, w_n\)</span> til et tilfældigt tal (f.eks. <span class="math inline">\(0.5\)</span>).</p></li>
<li><p>Tag et træningseksempel <span class="math inline">\((x_1, x_2, \dots, x_n)\)</span> med tilhørende target-værdi <span class="math inline">\(t\)</span>.</p></li>
<li><p>Udregn outputværdien <span class="math inline">\(o\)</span>: <span class="math display">\[
\begin{aligned}
o = \begin{cases}
1 &amp; \text{hvis } w_0 + w_1 \cdot x_1 + \cdots + w_n \cdot x_n \geq 0 \\
-1 &amp; \text{hvis } w_0 + w_1 \cdot x_1 + \cdots + w_n \cdot x_n &lt; 0. \\
\end{cases}
\end{aligned}
\]</span></p></li>
<li><p>Udregn fejlen <span class="math inline">\(error\)</span> (som er forskellen mellem den ønskede target-værdi <span class="math inline">\(t\)</span> og den beregnede outputværdi <span class="math inline">\(o\)</span>): <span class="math display">\[
\begin{aligned}
error = t-o
\end{aligned}
\]</span></p></li>
<li><p>Opdatér alle vægtene: <span class="math display">\[
\begin{aligned}
w_0 \leftarrow w_0 + &amp; \eta \cdot error \\
w_1 \leftarrow w_1 + &amp; \eta \cdot  error \cdot x_1 \\
w_2 \leftarrow w_2 + &amp; \eta \cdot  error \cdot x_2 \\
&amp; \vdots  \\
w_n \leftarrow w_n + &amp; \eta \cdot  error \cdot x_n \\
\end{aligned}
\]</span> Her er <span class="math inline">\(\eta\)</span> (udtales “eta”) et tal mellem <span class="math inline">\(0\)</span> og <span class="math inline">\(1\)</span>, som kaldes for en <em>learning rate</em> (på dansk: en læringsrate). Værdien af <span class="math inline">\(\eta\)</span> afgør hvor hurtigt, vi skal opdatere vægtene. Hvis <span class="math inline">\(\eta\)</span> er tæt på <span class="math inline">\(0\)</span> opdateres vægtene langsomt, hvorimod en værdi af <span class="math inline">\(\eta\)</span> tæt på <span class="math inline">\(1\)</span> vil betyde, at vægtene opdateres hurtigt.</p></li>
<li><p>Start forfra med det næste træningseksempel indtil værdien af vægtene ikke ændrer sig.</p></li>
</ul>
<p>Ovenstående fortsætter man altså med indtil, at vægtene ikke længere ændrer sig. Det vil altså sige, at man typisk vil komme tilbage og bruge det samme træningseksempel mange gange.</p>
<p>Nu kan man jo undre sig over, hvorfor perceptron learning algoritmen overhovedet virker, men det kommer vi tilbage til lige om lidt. Lad os først prøve den af på vores eget lille eksempel. Vi kan for overskuelighedens skyld starte med at stille vores træningsdata op i en tabel:</p>
<div class="cell page-columns page-full">
<div class="cell-output-display page-columns page-full">
<div id="tbl-trainingdata_tabel" class="anchored page-columns page-full">
<table class="table table-sm table-striped">

<colgroup>
<col style="width: 31%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Træningseksempel nr.</th>
<th style="text-align: center;"><span class="math inline">\(x_1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(x_2\)</span></th>
<th style="text-align: center;">Targetværdi <span class="math inline">\(t\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">-1</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">6</td>
<td style="text-align: center;">-2</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">-1</td>
</tr>
</tbody>
</table>
<div class="quarto-table-caption margin-caption">Tabel&nbsp;1: Træningsdata fra eksemplet om hvorvidt man skal stemme på blå eller rød blok.</div></div>
</div>
</div>
<p>Lad os nu prøve perceptron learning algoritmen af. Vi starter med at sætte vægtene til <span class="math inline">\(0.5\)</span>: <span class="math display">\[
\begin{aligned}
w_0 = 0.5, w_1=0.5, w_2=0.5
\end{aligned}
\]</span> og lad os også sætte vores <em>learning rate</em> til <span class="math inline">\(0.5\)</span>: <span class="math display">\[
\eta = 0.5.
\]</span> Så tager vi udgangspunkt i træningseksempel <span class="math inline">\(1\)</span> og udregner: <span class="math display">\[
\begin{aligned}
w_0+w_1\cdot x_1 + w_2 \cdot x_2 = 0.5 + 0.5 \cdot 2 + 0.5 \cdot 1 = 2
\end{aligned}
\]</span> Da denne værdi er positiv, bliver <span class="math inline">\(o=1\)</span>. Vores target-værdi <span class="math inline">\(t\)</span> er også <span class="math inline">\(1\)</span>. Det betyder, at med disse værdier af vægtene bliver træningseksempel nr. <span class="math inline">\(1\)</span> faktisk klassificeret korrekt. Derfor bliver fejlen også <span class="math display">\[
\begin{aligned}
error = t-o=1-1=0
\end{aligned}
\]</span> og derfor ender vi også med ikke at opdatere nogle af vægtene, fordi vi generelt har (hvor <span class="math inline">\(x_0=1\)</span>): <span class="math display">\[
\begin{aligned}
w_i \leftarrow w_i + \eta \cdot error \cdot x_i,
\end{aligned}
\]</span> men når <span class="math inline">\(error=0\)</span> ender vi bare med <span class="math display">\[
\begin{aligned}
w_i \leftarrow w_i.
\end{aligned}
\]</span> Altså ingen opdatering af vægtene. Det giver jo rigtig god mening, eftersom det første træningseksempel blev klassificeret korrekt.</p>
<p>Vi går nu videre til det næste træningseksempel og udregner: <span class="math display">\[
\begin{aligned}
w_0+w_1\cdot x_1 + w_2 \cdot w_2 = 0.5 + 0.5 \cdot (-1) + 0.5 \cdot 1 = 0.5.
\end{aligned}
\]</span> Da denne værdi er positiv, bliver <span class="math inline">\(o=1\)</span>. Men nu er vores target-værdi <span class="math inline">\(t=-1\)</span>. Det vil altså sige, at dette træningseksempel klassificeres forkert, og derfor bliver fejlen også <span class="math display">\[
\begin{aligned}
error = t-o=-1-1=-2.
\end{aligned}
\]</span> Vores vægte skal derfor opdateres til <span class="math display">\[
\begin{aligned}
w_0 &amp;\leftarrow 0.5 +0.5 \cdot (-2) = -0.5 \\
w_1 &amp;\leftarrow 0.5 +0.5 \cdot (-2) \cdot (-1) = 1.5 \\
w_2 &amp;\leftarrow 0.5 +0.5 \cdot (-2) \cdot 1= -0.5 \\
\end{aligned}
\]</span></p>
<p>Så går vi videre til træningseksempel <span class="math inline">\(3\)</span> og udregner igen (men nu med de nye vægte!): <span class="math display">\[
\begin{aligned}
w_0+w_1\cdot x_1 + w_2 \cdot w_2 = -0.5 +1.5 \cdot (-1) -0.5 \cdot (-1) = -1.5.
\end{aligned}
\]</span> Da denne værdi er negativ, bliver <span class="math inline">\(o=-1\)</span>. Men vores target-værdi er også <span class="math inline">\(t=-1\)</span>. Altså har vi igen en korrekt klassificering. Fejlen er derfor også: <span class="math display">\[
\begin{aligned}
error = t-o=-1-(-1)=0.
\end{aligned}
\]</span> Og ingen af vægtene skal derfor opdateres.</p>
<p>Sådan fortsætter man nu med træningseksempel nr. <span class="math inline">\(4, 5\)</span> og <span class="math inline">\(6\)</span>, og så starter man forfra igen med træningseksempel nr. <span class="math inline">\(1\)</span>, <span class="math inline">\(2\)</span> og så videre. Når man har været igennem en hel runde af de <span class="math inline">\(6\)</span> træningseksempler, hvor vægtene ikke har ændret sig, så stopper man! I vores lille eksempel kommer der faktisk ikke flere opdateringer af vægtene, og man ender derfor med følgende vægte: <span class="math display">\[
\begin{aligned}
w_0 = -0.5, w_1 = 1.5, w_2=-0.5
\end{aligned}
\]</span> Det svarer til ligningen <span class="math display">\[
\begin{aligned}
-0.5 +1.5 \cdot x_1 -0.5 \cdot x_2 = 0.
\end{aligned}
\]</span> Indtegner man linjen med denne ligning i et koordinatsystem sammen med vores træningsdata, og den linje vi selv “sjussede” os frem til, så kommer det til at se sådan her ud (den grønne linje er den, som kommer fra perception learning algoritmen):</p>
<div class="cell page-columns page-full">
<div class="cell-output-display page-columns page-full">
<div id="fig-punkter_kandidattest_graf3" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="perceptron-note_files/figure-html/fig-punkter_kandidattest_graf3-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption margin-caption">Figur&nbsp;5: Illustration af svaret på spørgsmål 1 (<span class="math inline">\(1.\)</span> aksen) og spørgsmål 2 (<span class="math inline">\(2.\)</span> aksen) med en markering af om man vil stemme på rød eller blok blå. Her er indtegnet to linjer, som kan separere de blå punkter fra de røde. Den sorte linje er vores eget skøn, mens den grønne stammer fra perception learning algoritmen (med startværdier <span class="math inline">\(w_0=0.5\)</span>, <span class="math inline">\(w_1=0.5\)</span>, <span class="math inline">\(w_2=0.5\)</span> og en learning rate <span class="math inline">\(\eta=0.5\)</span>).</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Det skal bemærkes, at hvis man vælger nogle andre startværdier af vægtene <span class="math inline">\(w_0, w_1\)</span> og <span class="math inline">\(w_2\)</span> end <span class="math inline">\(0.5\)</span>, så vil man som oftest også ende med nogle nye slutværdier af vægtene! Man vil stadig få en linje, som kan skille de blå punkter fra de røde punkter – det er bare en anden linje, som kan det samme! Hvis vi f.eks. som startværdier havde valgt <span class="math inline">\(w_0=-1.5, w_1=1.2\)</span> og <span class="math inline">\(w_2=1\)</span> (det svarer til vores eget oprindelige skøn - den sorte linje ovenfor), så vil alle træningseksemplerne allerede være klassificeret korrekt, dermed vil ingen af vægtene på noget tidspunkt blive opdateret (fordi fejlen hele tiden vil være 0), og vi vil i det tilfælde ende med den sorte linje som output fra algoritmen.</p>
<div id="exm-Rosenblatts2" class="theorem example page-columns page-full">
<p><span class="theorem-title"><strong>Eksempel 2 </strong></span><em>Vi vil nu se på, hvilken betydning de nye vægte får for vælgeren i <a href="#exm-Rosenblatts">eksempel&nbsp;1</a>. Vi har stadig, at <span class="math inline">\(x_1=0\)</span> og <span class="math inline">\(x_2=2\)</span>. Vi udregner nu med de nye vægte: <span class="math display">\[
-0.5+1.5 \cdot x_1-0.5 \cdot x_2=-0.5+1.5 \cdot 0-0.5 \cdot 2=-1.5
\]</span> Nu har vi en værdi, som er mindre end <span class="math inline">\(0\)</span>, og derfor sætter vi <span class="math inline">\(o=-1\)</span>. Det vil sige, at vores anbefaling til denne vælger nu vil være at stemme på rød blok! Det kan måske virke overraskende, men <a href="#fig-punkter_kandidattest_graf4">figur&nbsp;6</a> illustrerer hvorfor, vi ikke får det samme i de to tilfælde.</em></p>
<div class="cell page-columns page-full">
<div class="cell-output-display page-columns page-full">
<div id="fig-punkter_kandidattest_graf4" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="perceptron-note_files/figure-html/fig-punkter_kandidattest_graf4-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption margin-caption">Figur&nbsp;6: Illustration af svaret på spørgsmål 1 (<span class="math inline">\(1.\)</span> aksen) og spørgsmål 2 (<span class="math inline">\(2.\)</span> aksen) med en markering af om man vil stemme på rød eller blok blå. Her er indtegnet to linjer, som kan separere de blå punkter fra de røde. Den sorte linje er vores eget skøn, mens den grønne stammer fra perception learning algoritmen (med startværdier <span class="math inline">\(w_0=0.5\)</span>, <span class="math inline">\(w_1=0.5\)</span> og <span class="math inline">\(w_2=0.5\)</span> og en learning rate <span class="math inline">\(\eta=0.5\)</span>). Punktet med koordinatsæt <span class="math inline">\((0,2)\)</span> er markeret med sort.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><em>Det sorte punkt i <span class="math inline">\((0,2)\)</span> repræsenterer vælgerens svar på de to spørgsmål. Her kan vi se, at punktet ligger over den sorte linje svarende til, at vi med denne klassificering anbefaler vælgeren at stemme på blå blok. Samtidig ligger punktet til venstre for den grønne linje og bruges denne linje til klassificering, vil vi nu anbefale vælgeren at stemme på rød blok.</em></p>
</div>
<p>Men hvorfor virker skidtet? Altså hvorfor kan perceptron learning algoritmen bruges til at adskille de røde punkter fra de blå? Det er ikke umiddelbart oplagt, at denne måde at opdatere vægtene på, ender med at give os en linje, som kan separerer de blå punkter fra de røde. Så lad os prøve at se lidt på, hvorfor det kan give mening.</p>
<p>Vi har allerede i vores eksempel set, at hvis de vægte, vi står med nu, korrekt kan klassificere vores træningsdata – det vil sige, at <span class="math inline">\(t=o\)</span>, så bliver fejlen <span class="math display">\[
\begin{aligned}
error = t-o=0
\end{aligned}
\]</span> og vi kommer slet ikke til at opdatere vægtene, fordi vi bare ender med <span class="math display">\[
\begin{aligned}
w_i \leftarrow w_i.
\end{aligned}
\]</span> Hvis vi derimod står i den situation, at vores træningsdata ikke klassificeres korrekt, så vil <span class="math inline">\(t \neq o\)</span>. Der er her to muligheder: <span class="math display">\[
\begin{aligned}
\text{Mulighed 1:} \quad \quad &amp;t=1 \quad \text{og} \quad o=-1 \quad \Rightarrow \quad error = t-o=2 \\
\text{Mulighed 2:} \quad \quad &amp;t=-1 \quad \text{og} \quad o=1 \quad \Rightarrow \quad error = t-o=-2 \\
\end{aligned}
\]</span> Lad os starte med mulighed <span class="math inline">\(1\)</span>. Her er target-værdien <span class="math inline">\(t=1\)</span>, men outputværdien <span class="math inline">\(o\)</span> er <span class="math inline">\(-1\)</span>. Når outputværdien er <span class="math inline">\(-1\)</span>, er det fordi, at <span class="math display">\[
\begin{aligned}
w_0 + w_1 \cdot x_1 + \cdots + w_n \cdot x_n &lt; 0.
\end{aligned}
\]</span> Vi kalder lige venstre siden for <span class="math inline">\(y_{old}\)</span>: <span class="math display">\[
\begin{aligned}
y_{old} = w_0 + w_1 \cdot x_1 + \cdots + w_n \cdot x_n.
\end{aligned}
\]</span> Vi ville egentlig gerne have, at denne værdi <span class="math inline">\(y_{old}\)</span> havde været positiv, fordi så ville vi have fået en rigtig klassificering. Vi ser nu på, hvad der sker efter opdateringen af vægtene. Da <span class="math inline">\(error=2\)</span> i mulighed <span class="math inline">\(1\)</span> har vi opdateringsreglen (hvor <span class="math inline">\(x_0=1\)</span>): <span class="math display">\[
\begin{aligned}
w_i \leftarrow w_i + \eta \cdot error \cdot x_i = w_i + \eta \cdot 2 \cdot x_i.
\end{aligned}
\]</span> Det vil sige, at næste gang vi skal beregne vores outputværdi, så skal vi bruge de nye vægte og udregne (vi kalder nu den nye værdi for <span class="math inline">\(y_{new}\)</span>): <span class="math display">\[
\begin{aligned}
y_{new} = (w_0+\eta \cdot 2) + (w_1+\eta \cdot 2 \cdot x_1) \cdot x_1 + \cdots + (w_n+\eta \cdot 2 \cdot x_n) \cdot x_n,
\end{aligned}
\]</span> hvor vi her har indsat de opdaterede vægte. Ganger vi ind i parenteserne får vi <span class="math display">\[
\begin{aligned}
y_{new} &amp;= w_0+\eta \cdot 2 + w_1\cdot x_1 + \eta \cdot 2\cdot x_1^2 + \cdots + w_n \cdot x_n + \eta \cdot 2\cdot x_n^2 \\
&amp;=w_0 + w_1 \cdot x_1 + \cdots + w_n \cdot x_n + \eta \cdot 2 + \eta \cdot 2\cdot x_1^2 + \cdots +\eta \cdot 2\cdot x_n^2 \\
&amp;=y_{old} + \eta \cdot 2 \cdot(1 + x_1^2 + \cdots +x_n^2).
\end{aligned}
\]</span> Men da <span class="math inline">\(\eta &gt;0\)</span> må <span class="math display">\[
\begin{aligned}
\eta \cdot 2 \cdot (1 + x_1^2 + \cdots +x_n^2) &gt;0,
\end{aligned}
\]</span> og derfor er <span class="math display">\[
\begin{aligned}
y_{new} &gt; y_{old}.
\end{aligned}
\]</span> Der er ingen, der siger, <span class="math inline">\(y_{new}\)</span> er blevet positiv, som ønsket, men denne værdi er i hvert til fælde tættere på at være positiv, end den gamle værdi var. Derfor vil vi under alle omstændigheder være lidt tættere på at klassificere dette træningseksempel korrekt, end vi var før.</p>
<p>I mulighed <span class="math inline">\(2\)</span> er target-værdien <span class="math inline">\(t=-1\)</span>, mens outputværdien <span class="math inline">\(o\)</span> er <span class="math inline">\(1\)</span>. Det må så betyde, at <span class="math display">\[
\begin{aligned}
w_0 + w_1 \cdot x_1 + \cdots + w_n \cdot x_n \geq 0,
\end{aligned}
\]</span> hvor vi egentlig gerne ville have haft, at denne værdi havde været negativ. I mulighed <span class="math inline">\(2\)</span> er fejlen <span class="math inline">\(error=-2\)</span> og opdateringsreglen bliver derfor <span class="math display">\[
\begin{aligned}
w_i \leftarrow w_i + \eta \cdot error \cdot x_i = w_i -\eta \cdot 2 \cdot x_i.
\end{aligned}
\]</span> Med et helt tilsvarende argument som ovenfor, kan man nu vise, at <span class="math display">\[
\begin{aligned}
y_{new} &lt; y_{old}
\end{aligned}
\]</span> og igen er det ikke sikkert, at <span class="math inline">\(y_{new}\)</span> er blevet negativ, men vi er i hvert tilfælde tættere på, end vi var før.</p>
<p>Ovenstående giver en intuitiv fornemmelse af, hvorfor perceptron learning algoritmen virker. Men det er stadig ikke klart, hvorfor det hele ender godt. Vi justerer jo hele tiden vægtene "til fordel for" ét træningseksempel ad gangen, og hvorfor kunne man ikke ende i en situation, hvor den næste justering baseret på det næste træningseksempel fuldstændig ødelægger den justering vi lige har lavet, så det første træningseksempel igen ikke nærmer sig en korrekt klassificering? Se det kræver et bevis at vise, at det hele går godt. Og det går faktisk heller ikke altid godt! Men man kan vise, at hvis de røde og de blå punkter ligger sådan, at de kan adskilles af en ret linje (man siger, at punkterne er <em>lineært separable</em>), så finder perceptron learning algoritmen altid en linje, som adskiller punkterne. Men hvis det ikke er tilfældet – altså hvis de røde og de blå punkter <em>ikke</em> kan adskilles af en ret linje – så går perceptron learning algoritmen i selvsving! Den kommer simpelthen aldrig frem til nogle værdier af vægtene, som bare tilnærmelsesvist kan bruges til at adskille punkterne. Man siger, at algoritmen ikke konvergerer.</p>
<section id="video-perceptron-learning-algoritmen" class="level2">
<h2 class="anchored" data-anchor-id="video-perceptron-learning-algoritmen">VIDEO: Perceptron Learning Algoritmen</h2>
<p>I denne video forklarer vi perceptron learning algoritmen.</p>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/xCeSZgdU3jc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
</section>
<section id="det-kan-gøres-smartere-med-adaline" class="level1 page-columns page-full">
<h1>Det kan gøres smartere med “Adaline”</h1>
<p>Det er faktisk lidt noget rod. Altså det her med, at perceptron learning algoritmen ikke kan finde meningsfulde værdier af vægtene, hvis de røde og de blå punkter ikke kan adskilles af en ret linje. Det er der to grunde til:</p>
<ol type="1">
<li><p>Hvis der er små fejl i data – eksempelvis en targetværdi der er forkert. Et eksempel herpå ses i figur <a href="#fig-punkter_graf4_5-1">figur&nbsp;7 (a)</a>. Her er et enkelt datapunkt fejlagtigt farvet rødt. Denne ene fejl får den konsekvens,at datapunkterne ikke længere er lineært separable. Perceptron learning algoritmen vil derfor ikke konvergere. Her ville det være smart med en algoritme, som vil give et fornuftigt svar, selvom der er små fejl i datasættet.</p></li>
<li><p>Verden er sjældent lineær separabel! Antag, at der ingen fejl er i datasættet i figur <a href="#fig-punkter_graf4_5-1">figur&nbsp;7 (a)</a>. Alligevel vil det stadig give mening at indtegne en ret linje som i figur <a href="#fig-punkter_graf4_5-2">figur&nbsp;7 (b)</a>, der kan bruges til klassificering. Med en enkelt undtagelse vil man klassificere alle træningsdata korrekt ved hjælp af denne linje.</p></li>
</ol>
<div id="fig-punkter_graf4_5" class="cell quarto-layout-panel page-columns page-full">
<p></p><figcaption>Figur&nbsp;7: Træningsdata som ikke er lineært separable.</figcaption><p></p>
<figure class="figure page-columns page-full">
<div class="quarto-layout-row quarto-layout-valign-top page-columns page-full">
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref page-columns page-full" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-punkter_graf4_5-1" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="perceptron-note_files/figure-html/fig-punkter_graf4_5-1.png" class="img-fluid figure-img" data-ref-parent="fig-punkter_graf4_5" width="672"></p>
<p></p><figcaption class="figure-caption margin-caption">(a) Træningsdata hvor et af datapunkterne ved en fejl har fået en forkert targetværdi.</figcaption><p></p>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref page-columns page-full" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-punkter_graf4_5-2" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="perceptron-note_files/figure-html/fig-punkter_graf4_5-2.png" class="img-fluid figure-img" data-ref-parent="fig-punkter_graf4_5" width="672"></p>
<p></p><figcaption class="figure-caption margin-caption">(b) Træningsdata hvor stort set alle datapunkterne er klassificeret korrekt.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</figure>
</div>
<p>Der er derfor brug for noget lidt andet end den klassiske perceptron, som Rosenblatt foreslog. Eller rettere sagt: perceptronen er god nok, det er faktisk bare læringsalgoritmen, som skal justeres lidt. Det viser sig nemlig, at problemet opstår fordi vi i læringsalgoritmen bruger det diskrete output <span class="math inline">\(o\)</span>: <span id="eq-output_perceptron"><span class="math display">\[
\begin{aligned}
o = \begin{cases}
1 &amp; \text{hvis } w_0 + w_1 \cdot x_1 + \cdots + w_n \cdot x_n \geq 0 \\
-1 &amp; \text{hvis } w_0 + w_1 \cdot x_1 + \cdots + w_n \cdot x_n &lt; 0. \\
\end{cases}
\end{aligned}
\tag{2}\]</span></span> i stedet for den vægtede sum <span class="math display">\[
\begin{aligned}
w_0 + w_1 \cdot x_1 + \cdots + w_n \cdot x_n.
\end{aligned}
\]</span> Hvis for eksempel vi har et træningsdatasæt med en targetværdi på <span class="math inline">\(t=1\)</span>, som fejlagtigt klassificeres med <span class="math inline">\(o=-1\)</span>. Så er der i perceptron learning algoritmen ingen forskel på om <span class="math display">\[
\begin{aligned}
w_0 + w_1 \cdot x_1 + \cdots + w_n \cdot x_n = -0.1
\end{aligned}
\]</span> eller om <span class="math display">\[
\begin{aligned}
w_0 + w_1 \cdot x_1 + \cdots + w_n \cdot x_n = -5000.
\end{aligned}
\]</span> I det første tilfælde er vægtene jo tæt på at være gode nok (den vægtede sum er jo næsten positiv), hvorimod de er meget langt fra at være gode nok i det andet tilfælde. Men det er perceptron learning algoritmen fuldstændig ligeglad med – begge tilfælde bliver behandlet <em>helt</em> ens! Så hvis vi skal rette op på perceptron learning algoritmen må vi gå fra at se på fejlen <span class="math display">\[
\begin{aligned}
error = t-o
\end{aligned}
\]</span> til at se på <span class="math display">\[
\begin{aligned}
error = t - (w_0 + w_1 \cdot x_1 + \cdots + w_n \cdot x_n)
\end{aligned}
\]</span> Bemærk, at i vores lille eksempel så vil perceptron learning algoritmen give fejlen <span class="math display">\[
\begin{aligned}
error = t-o=1-(-1)=2
\end{aligned}
\]</span> i begge tilfælde. Hvis vi i stedet bruger <span class="math inline">\(error = t - (w_0 + w_1 \cdot x_1 + \cdots + w_n \cdot x_n)\)</span> får vi: <span class="math display">\[
\begin{aligned}
&amp;\text{Tilfælde 1:} \quad error = 1-(-0.1)=1.1 \\
&amp;\text{Tilfælde 2:} \quad error = 1-(-5000)=5001
\end{aligned}
\]</span> Her bliver det meget tydeligt, at fejlen er lille i tilfælde 1 og stor i tilfælde 2. Hvis vi skal tage højde for det i vores opdateringsalgoritme, så må vi altså væk fra den meget simple måde, at definere fejlen på og i stedet se på fejlen som: <span class="math inline">\(error = t - (w_0 + w_1 \cdot x_1 + \cdots + w_n \cdot x_n)\)</span>. Dette er præcis, hvad man gør i ADAPTIVE LINEAR neuroner (Adaline).</p>
<p>For at forstå læringsalgoritmen i Adaline må vi først indføre en tabsfunktion, som skal give et samlet mål for, hvor god/dårlig perceptronen klassificerer alle træningsdata for givne værdier af vægtene. Men først er vi nødt til at indføre lidt ny notation, så vi kan skelne de forskellige træningsdata fra hinanden. Antag, at vi har <span class="math inline">\(M\)</span> træningseksempler. Så vil vi opskrive de <span class="math inline">\(M\)</span> træningseksempler på denne måde: <span class="math display">\[
\begin{aligned}
&amp;\text{Træningseksempel 1:} \quad (x_{1,1}, x_{1,2}, \dots, x_{1,n}, t_1) \\
&amp;  \quad \quad \quad \quad \vdots \\
&amp;\text{Træningseksempel m:} \quad (x_{m,1}, x_{m,2}, \dots, x_{m,n}, t_m) \\
&amp;  \quad \quad \quad \quad \vdots \\
&amp;\text{Træningseksempel M:} \quad (x_{M,1}, x_{M,2}, \dots, x_{M,n}, t_M) \\
\end{aligned}
\]</span> Det vil altså sige, at det <span class="math inline">\(m\)</span>’te træningseksempel har inputværdier <span class="math inline">\((x_{m,1}, x_{m,2}, \dots, x_{m,n})\)</span> og targetværdi <span class="math inline">\(t_m\)</span>.</p>
<p>Vi definerer nu tabsfunktionen <span class="math inline">\(E\)</span> på følgende måde: <span id="eq-tabsfunktion"><span class="math display">\[
\begin{aligned}
E(w_0, w_1, \dots, w_n) = \frac{1}{2} \sum_{m=1}^{M} \left (t_m-
(w_0 + w_1 \cdot x_{m,1} + \cdots + w_n \cdot x_{m,n}) \right)^2
\end{aligned}
\tag{3}\]</span></span> Men hvorfor giver det nu god mening? Jo, det gør det, fordi udtrykket i parentesen <span class="math display">\[
\begin{aligned}
t_m-(w_0 + w_1 \cdot x_{m,1} + \cdots + w_n \cdot x_{m,n})
\end{aligned}
\]</span> angiver fejlen, som vi så det i eksemplet ovenfor. Det er altså forskellen mellem den ønskede targetværdi <span class="math inline">\(t_m\)</span> for det <span class="math inline">\(m\)</span>’te træningseksempel og den vægtede sum <span class="math display">\[
\begin{aligned}
w_0 + w_1 \cdot x_{m,1} + \cdots + w_n \cdot x_{m,n},
\end{aligned}
\]</span> som perceptronen beregner, <em>inden</em> vi finder den endelig outputværdi <span class="math inline">\(o\)</span> ved at benytte (<a href="#eq-output_perceptron">2</a>). Denne forskel kan være både positiv og negativ – og et stort positivt tal er lige så "slemt", som det tilsvarende negative tal. Derfor sætter man forskellen i anden for at slippe af med fortegnene. Endelig lægger vi alle de kvadrerede fejlled fra alle de <span class="math inline">\(M\)</span> træningsdata sammen (og ganger med en halv – det er ikke så afgørende – du ser lige om lidt hvorfor). Hvis vores perceptron med de nuværende værdier af vægtene er god til at klassificere alle træningseksemplerne, så vil vores tabsfunktion give en lille værdi og omvendt, hvis perceptronen er dårlig til at klassificere, vil tabsfunktionen give en stor værdi. Se det er faktisk hele humlen. Vi skal finde de værdier af vægtene <span class="math inline">\(w_0, w_1, \dots, w_n\)</span>, som minimerer tabsfunktionen!</p>
<section id="video-perceptron-learning-versus-adaline" class="level2">
<h2 class="anchored" data-anchor-id="video-perceptron-learning-versus-adaline">VIDEO: Perceptron Learning versus Adaline</h2>
<p>I denne video forklarer vi idéen bag Adaline og indfører tabsfunktionen.</p>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/QF3WRIe5v6U" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
<section id="gradientnedstigning" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="gradientnedstigning">Gradientnedstigning</h2>
<p>For at gøre det bruges en metode, som kaldes for <em>gradientnedstigning</em>. For at forklare hvad det går ud på, er det nemmest at se på en tabsfunktion, som kun afhænger af to vægte <span class="math inline">\(w_0\)</span> og <span class="math inline">\(w_1\)</span>. I det tilfælde får vi <span class="math display">\[
\begin{aligned}
E(w_0, w_1) = \frac{1}{2} \sum_{m=1}^{M} \left (t_m-
(w_0 + w_1 \cdot x_{m,1}) \right)^2.
\end{aligned}
\]</span> Da tabsfunktionenkun afhænger af to variable, kan vi tegne grafen for den. Et eksempel herpå ses i <a href="#fig-tabsfunktion">figur&nbsp;8</a>.</p>
<div id="fig-tabsfunktion" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/tabsfunktion.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption margin-caption">Figur&nbsp;8: Grafen for en tabsfunktion som afhænger af vægtene <span class="math inline">\(w_0\)</span> og <span class="math inline">\(w_1\)</span>.</figcaption><p></p>
</figure>
</div>
<p>Idéen er nu, at vi gerne vil bestemme vægtene <span class="math inline">\(w_0\)</span> og <span class="math inline">\(w_1\)</span>, sådan at tabsfunktionen minimeres. Tænk lidt over det. Det giver god mening, at bestemme vægtene sådan at den samlede fejl, perceptronen begår på træningsdata, bliver så lille som mulig. Vi ved faktisk godt, hvordan man bestemmer minimum for en funktion af to variable. Løs ligningerne <span class="math display">\[
\begin{aligned}
\frac{\partial E}{\partial w_0} = 0 \quad \text{og} \quad \frac{\partial E}{\partial w_1} = 0.\end{aligned}
\]</span> Det er en overkommelig opgave at finde de partielle afledede og sætte dem lig med <span class="math inline">\(0\)</span> i det tilfælde, hvor tabsfunktionen kun afhænger af to vægte. Men vi skal senere se, at perceptroner bliver fundamentale byggesten i kunstige neurale netværk, og her viser det sig, at denne fremgangsmåde med at sætte de partielle afledede lig <span class="math inline">\(0\)</span>, er helt håbløs! Derfor bruger man gradientnedstigning.</p>
<p>Forestil dig at grafen for tabsfunktionen i <a href="#fig-tabsfunktion">figur&nbsp;8</a> er et landskab med en dal. Dit mål er at finde ned i dalen. Du er blevet placeret et tilfældigt sted i landskabet svarende til tilfældige værdier af <span class="math inline">\(w_0\)</span> og <span class="math inline">\(w_1\)</span>. Hvad gør du? Jo, du kommer i tanke om, at du har lært, at hvis du går i gradientens <span class="math display">\[
\begin{aligned}
\nabla E(w_0,w_1) = \begin{pmatrix} \frac{\partial E }{\partial w_0}(w_0,w_1) \\ \\ \frac{\partial E }{\partial w_1}(w_0,w_1) \end{pmatrix}
\end{aligned}
\]</span> retning, så kommer du til at gå i den retning, hvor det går <em>allermest</em> opad bakke! Men hov det er jo ikke det, vi vil! Vi vil gå allermest nedad bakke, så vi ender i dalen. Hvad gør vi så? Vi vender os da bare <span class="math inline">\(180^{\circ}\)</span> og går i den modsatte retning - så ender vi nede i dalen! Det vil sige, at vi går i retning af minus gradienten: <span class="math display">\[
\begin{aligned}
- \nabla E(w_0,w_1) = \begin{pmatrix} - \frac{\partial E }{\partial w_0}(w_0,w_1) \\ \\ - \frac{\partial E }{\partial w_1}(w_0,w_1) \end{pmatrix}
\end{aligned}
\]</span> Fremgangsmåden bliver derfor den, at vi starter i nogle tilfældige <span class="math inline">\((w_0, w_1)\)</span>-værdier og så bevæger vi os et lille skridt i den negative gradients retning. Så ender vi i et nyt punkt, hvor vi igen beregner gradienten og går igen et lille skridt i den negative gradients retning. Sådan fortsætter vi indtil værdien af tabsfunktionen ikke rigtig ændrer sig mere – det svarer til at vi har ramt dalen. Derfor bliver vores opdateringsregler med denne metode <span class="math display">\[
\begin{aligned}
w_0 \leftarrow &amp; w_0 - \eta \cdot \frac{\partial E }{\partial w_0} \\
w_1 \leftarrow &amp; w_1 - \eta \cdot \frac{\partial E }{\partial w_1} \\
&amp;\vdots  \\
w_n \leftarrow &amp; w_n - \eta \cdot \frac{\partial E }{\partial w_n} \\
\end{aligned}
\]</span> Her er <span class="math inline">\(\eta\)</span> igen en <em>learning rate</em> f.eks. <span class="math inline">\(0.05\)</span>, som sørger for, at vi hele tiden bare tager et lille skridt i den negative gradients retning. Man vælger værdien af <span class="math inline">\(\eta\)</span> lille for ikke at lave alt for store justeringer af vægtene ad gangen. Det svarer grafisk til, at vi lige så stille går ned af den bakke, som tabsfunktionen giver (se <a href="#fig-tabsfunktion">figur&nbsp;8</a>). Hvis vi tager for store skridt, risikerer vi helt, at komme til at “træde forbi” det minimum, som vi gerne vil lande i. Omvendt vil alt for små skridt føre til, at vi alt for langsomt nærmer os minimum. Så værdien af <span class="math inline">\(\eta\)</span> angiver altså, hvor meget vi er villige til at justere vægtene og dermed hvor hurtige eller hvor langsomme, vi bevæger os ned mod minimum. Af den grund giver det god mening, at <span class="math inline">\(\eta\)</span> kaldes for en <em>learning rate</em> - fordi den afgører, hvor hurtigt vi lærer af vores træningsdata.</p>
<p>Nu mangler vi bare at få bestemt de partielle afledede. Ved at bruge sumreglen og kædereglen for differentiation får vi fra (<a href="#eq-tabsfunktion">3</a>), at <span class="math display">\[
\begin{aligned}
\frac{\partial E}{\partial w_i} &amp;= \frac{1}{2} \sum_{m=1}^{M} \frac{\partial}{\partial w_i}\left (t_m-
(w_0 + w_1 \cdot x_{m,1} + \cdots + w_n \cdot x_{m,n}) \right)^2 \\
&amp;= \frac{1}{2} \sum_{m=1}^{M} 2 \cdot \left (t_m-
(w_0 + w_1 \cdot x_{m,1} + \cdots + w_n \cdot x_{m,n}) \right) \\ &amp; \quad  \quad \quad  \quad \quad  \quad \cdot \frac{\partial}{\partial w_i} \left (t_m-
(w_0 + w_1 \cdot x_{m,1} + \cdots + w_n \cdot x_{m,n} ) \right) \\
&amp;= \sum_{m=1}^{M} \left (t_m-
(w_0 + w_1 \cdot x_{m,1} + \cdots + w_n \cdot x_{m,n}) \right) \cdot \left (-x_{m,i} \right)
\end{aligned}
\]</span> for <span class="math inline">\(i \in \{1, 2, \dots, n\}\)</span>.</p>
<p>Læg mærke til at når vi differentierer den indre funktion <span class="math display">\[
\begin{aligned}
t_m-(w_0 + w_1 \cdot x_{m,1} + \cdots + w_n \cdot x_{m,n})
\end{aligned}
\]</span> med hensyn til <span class="math inline">\(w_i\)</span>, så vil alle led være at betragte som konstanter bortset fra leddet <span class="math display">\[
\begin{aligned}
-w_i \cdot x_{m,i}
\end{aligned}
\]</span> og når vi differentierer dette led med hensyn til <span class="math inline">\(w_i\)</span> får vi netop <span class="math inline">\(- x_{m,i}\)</span>. Læg også mærke til at hvis vi differentierer med hensyn til <span class="math inline">\(w_0\)</span>, så får vi, <span class="math display">\[
\begin{aligned}
\frac{\partial E}{\partial w_0} = \sum_{m=1}^{M} \left (t_m-
(w_0 + w_1 \cdot x_{m,1} + \cdots + w_n \cdot x_{m,n}) \right) \cdot \left (-1 \right).
\end{aligned}
\]</span> Altså bliver vores opdateringsregler <span class="math display">\[
\begin{aligned}
w_0 \leftarrow &amp; w_0 + \eta \cdot \sum_{m=1}^{M} \left (t_m-
(w_0 + w_1 \cdot x_{m,1} + \cdots + w_n \cdot x_{m,n}) \right)  \\
w_1 \leftarrow &amp; w_1 + \eta \cdot  \sum_{m=1}^{M} \left (t_m-
(w_0 + w_1 \cdot x_{m,1} + \cdots + w_n \cdot x_{m,n}) \right) \cdot \left (x_{m,1} \right)\\
&amp;\vdots  \\
w_n \leftarrow &amp; w_n + \eta \cdot  \sum_{m=1}^{M} \left (t_m-
(w_0 + w_1 \cdot x_{m,1} + \cdots + w_n \cdot x_{m,n}) \right) \cdot \left (x_{m,n} \right)\\
\end{aligned}
\]</span> Vi kan altså sammenfatte gradientnedstigningsalgoritmen på denne måde:</p>
<ul>
<li><p>Sæt alle vægte <span class="math inline">\(w_0, w_1, \dots, w_n\)</span> til et tilfældigt tal (f.eks. <span class="math inline">\(0.5\)</span>).</p></li>
<li><p>Vælg en værdi af <span class="math inline">\(\eta\)</span> (f.eks. <span class="math inline">\(0.05\)</span>).</p></li>
<li><p>Udregn på baggrund af alle træningsdata fejlene: <span class="math display">\[
\begin{aligned}
error_0 &amp;= \sum_{m=1}^{M} \left (t_m-(w_0 + w_1 \cdot x_{m,1} + \cdots + w_n \cdot x_{m,n}) \right) \\
error_1 &amp;= \sum_{m=1}^{M} \left (t_m-(w_0 + w_1 \cdot x_{m,1} + \cdots + w_n \cdot x_{m,n}) \cdot x_{m,1} \right) \\
&amp;\vdots \\
error_n &amp;= \sum_{m=1}^{M} \left (t_m-(w_0 + w_1 \cdot x_{m,1} + \cdots + w_n \cdot x_{m,n}) \cdot x_{m,n} \right)
\end{aligned}
\]</span></p></li>
<li><p>Opdatér alle vægtene: <span class="math display">\[
\begin{aligned}
w_0  \leftarrow &amp; w_0 + \eta \cdot error_0 \\
w_1  \leftarrow &amp; w_1 + \eta \cdot error_1 \\
&amp; \vdots \\
w_n  \leftarrow &amp; w_n + \eta \cdot error_n
\end{aligned}
\]</span></p></li>
<li><p>Start forfra indtil vægtene ikke ændrer sig (særlig meget).</p></li>
</ul>
</section>
<section id="eksempel-på-gradientnedstigning" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="eksempel-på-gradientnedstigning">Eksempel på gradientnedstigning</h2>
<p>Lad os prøve at bruge gradientnedstigning på vores eksempel omkring kandidattest. I dette simple eksempel bliver vores opdateringsregler nu <span class="math display">\[
\begin{aligned}
w_0 \leftarrow &amp; w_0 + \eta \cdot \sum_{m=1}^{6} \left (t_m-
(w_0 + w_1 \cdot x_{m,1} + w_2 \cdot x_{m,2}) \right)  \\
w_1 \leftarrow &amp; w_1 + \eta \cdot  \sum_{m=1}^{6} \left (t_m-
(w_0 + w_1 \cdot x_{m,1} + w_2 \cdot x_{m,2}) \right) \cdot \left (x_{m,1} \right)\\
w_2 \leftarrow &amp; w_2 + \eta \cdot  \sum_{m=1}^{6} \left (t_m-
(w_0 + w_1 \cdot x_{m,1}  + w_2 \cdot x_{m,2}) \right) \cdot \left (x_{m,2} \right)
\end{aligned}
\]</span> Hvis man bruger disse opdateringsregler på dataene fra <a href="#tbl-trainingdata_tabel">tabel&nbsp;1</a>, så ender man med følgende værdier af vægtene <span class="math display">\[
\begin{aligned}
w_0 =-0.0769 , w_1=0.6410, w_2=-0.0598
\end{aligned}
\]</span> hvor vi startede med værdierne <span class="math inline">\(w_0=0.5, w_1=0.5\)</span> og <span class="math inline">\(w_2=0.5\)</span> og hvor vi satte <span class="math inline">\(\eta=0.05\)</span>. Dette svarer til linjen med ligning <span class="math display">\[
\begin{aligned}
-0.0769 + 0.6410 \cdot x_1 - 0.0598 \cdot x_2 = 0.
\end{aligned}
\]</span> Linjen er indtegnet sammen med træningsdata i <a href="#fig-punkter_kandidattest_gradientdescent">figur&nbsp;9</a>.</p>
<div class="cell page-columns page-full">
<div class="cell-output-display page-columns page-full">
<div id="fig-punkter_kandidattest_gradientdescent" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="perceptron-note_files/figure-html/fig-punkter_kandidattest_gradientdescent-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption margin-caption">Figur&nbsp;9: Ilustration af svaret på spørgsmål 1 (<span class="math inline">\(1.\)</span> aksen) og spørgsmål 2 (<span class="math inline">\(2.\)</span> aksen) med en markering af om man vil stemme på rød eller blok blå. Her er den linje indtegnet, som stammer fra gradientnedstigningsalgoritmen (med startværdier <span class="math inline">\(w_0=0.5, w_1=0.5, w_2=0.5\)</span> og <span class="math inline">\(\eta=0.05\)</span>).</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Bemærk, at til forskel fra perceptron learning algoritmen så ender man med de samme værdier af vægtene, selvom man vælger andre startværdier. Det er fordi, vi finder et globalt minimum for tabsfunktionen, som er uafhængig af det punkt, hvor vi starter med at lede. Det svarer til, at ligegyldigt hvor du bliver placeret i landskabet i <a href="#fig-tabsfunktion">figur&nbsp;8</a>, så vil du til sidst ende i dalen, hvis du hele tiden går små skridt i den negative gradients retning.</p>
<div id="exm-Rosenblatts3" class="theorem example">
<p><span class="theorem-title"><strong>Eksempel 3 </strong></span><em>Vi ser igen på <a href="#exm-Rosenblatts">eksempel&nbsp;1</a> og <a href="#exm-Rosenblatts2">eksempel&nbsp;2</a>, hvor <span class="math inline">\(x_1=0\)</span> og <span class="math inline">\(x_2=1\)</span>. Baseret på vægtene fra gradientnedstigning, får vi: <span class="math display">\[
-0.0769+0.6410 \cdot x_1 -0.0598 \cdot x_2=-0.0769+0.6410 \cdot 0 -0.0598 \cdot 1=-0.1367
\]</span> Da denne værdi er mindre end <span class="math inline">\(0\)</span>, sætter vi <span class="math inline">\(o=-1\)</span>. Altså er vi tilbage til at befale vælgeren at stemme på rød blok.</em></p>
</div>
</section>
<section id="forskel-på-perceptron-learning-og-gradientnedstigning" class="level2">
<h2 class="anchored" data-anchor-id="forskel-på-perceptron-learning-og-gradientnedstigning">Forskel på perceptron learning og gradientnedstigning</h2>
<p>Der er flere forskelle på perceptron learning algoritmen og gradientnedstigning. Vi har allerede været inde på, at perceptron learning algoritmen går i kuk, hvis data ikke er lineært separable, som i <a href="#fig-punkter_graf4_5-2">figur&nbsp;7 (b)</a>. Mere formelt vil perceptron learning algoritmen ikke konvergere. I det tilfælde vil gradientnedstigning alligevel komme med et bud på værdier af vægtene, som kan bruges til at kategorisere træningsdata, selvom alle træningsdata ikke vil blive klassificeret korrekt (fordi de netop ikke er lineært separable). En anden forskel ligger i hvornår vægtene opdateres. I perceptron learning algoritmen opdateres vægtene efter <em>hvert</em> træningseksempel. I gradientnedstigning bruger man <em>alle</em> træningsdata for at lave en enkelt opdatering af vægtene. Hvis man har mange træningsdata, kan det godt blive lidt tungt. Så kan man i stedet for vælge at bruge et mindre, tilfældigt udvalg af data (for eksempel <span class="math inline">\(10\%\)</span>) til hver opdatering og så til næste opdatering bruge et nyt tilfældigt udvalg af data. Denne fremgangsmåde kaldes for <em>stokastisk gradientnedstigning</em>. En anden mulighed er at lave <em>online</em> gradientnedstigning, hvor man opdaterer vægtene for hvert træningseksempel, som i perceptron learning algoritmen. Selvom det kun er en tilnærmelse til rigtig gradientnedstigning, så har det alligevel en række fordele: 1) Det er meget hurtigere at opdatere vægtene. 2) I nogle anvendelser vil man have brug for løbende at opdatere vægtene på baggrund af nye træningsdata. I stedet for at gemme alle de mange træningsdata kan man bare opdatere vægtene, hver gang man får et nyt træningseksempel til rådighed og så eventuelt slette træningseksemplet igen, når vægtene er blevet opdateret. Det er både hurtigere og mere pladsbesparende.</p>
</section>
<section id="video-adaline" class="level2">
<h2 class="anchored" data-anchor-id="video-adaline">VIDEO: Adaline</h2>
<p>I denne video forklarer vi, hvad gradientnedstigning går ud på, og hvordan gradientnedstigning bruges til at opdatere vægtene i Adaline.</p>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/4YMzpNP4wnA" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
</section>
<section id="noten-som-pdf" class="level1">
<h1>Noten som pdf</h1>
<p><a href="./perceptron-note.pdf">Perceptron note</a>.</p>
</section>
<section id="videre-læsning" class="level1">
<h1>Videre læsning</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-baktoft" class="csl-entry" role="doc-biblioentry">
Baktoft, Allan. 2014. <em>Matematik i Virkeligheden. Bind 2</em>. Forlaget Natskyggen.
</div>
<div id="ref-bishop" class="csl-entry" role="doc-biblioentry">
Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning</em>. Springer.
</div>
<div id="ref-chandra_MPneuron" class="csl-entry" role="doc-biblioentry">
Chandra, Akshay L. 2018a. <em>McCulloch-Pitts Neuron – Mankind’s First Mathematical Model of a Biological Neuron</em>. <a href="https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5dd1">https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5dd1</a>.
</div>
<div id="ref-chandra_perceptron_learning" class="csl-entry" role="doc-biblioentry">
———. 2018b. <em>Perceptron Learning Algorithm: A Graphical Explanation of Why It Works</em>. <a href="https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975">https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975</a>.
</div>
<div id="ref-chandra_perceptron" class="csl-entry" role="doc-biblioentry">
———. 2018c. <em>Perceptron: The Artificial Neuron (an Essential Upgrade to the McCulloch-Pitts Neuron)</em>. <a href="https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d">https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d</a>.
</div>
<div id="ref-Loiseau_perceptron" class="csl-entry" role="doc-biblioentry">
Loiseau, Jean-Christophe B. 2019. <em>Rosenblatt’s Perceptron, the First Modern Neural Network. A Quick Introduction to Deep Learning for Beginners</em>. <a href="https://towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a">https://towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a</a>.
</div>
<div id="ref-Loiseau_improving" class="csl-entry" role="doc-biblioentry">
———. 2020. <em>Improving Upon Rosenblatt’s Perceptron. Adaptive Linear Neurons and the Delta Rule</em>. <a href="https://towardsdatascience.com/improving-upon-rosenblatts-perceptron-d0517d3c5939">https://towardsdatascience.com/improving-upon-rosenblatts-perceptron-d0517d3c5939</a>.
</div>
<div id="ref-mitchell" class="csl-entry" role="doc-biblioentry">
Mitchell, Tom M. 1997. <em>Machine Learning</em>. The McGraw-Hill Companies, Inc.
</div>
<div id="ref-nielsen" class="csl-entry" role="doc-biblioentry">
Nielsen, Michael A. 2015. <em>Neural Networks and Deep Learning</em>. Determination Press. <a href="http://neuralnetworksanddeeplearning.com/index.html">http://neuralnetworksanddeeplearning.com/index.html</a>.
</div>
<div id="ref-raschka" class="csl-entry" role="doc-biblioentry">
Raschka, Sebstian. n.d. <em>Single-Layer Neural Networks and Gradient Descent</em>. <a href="https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html">https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html</a>.
</div>
<div id="ref-roy" class="csl-entry" role="doc-biblioentry">
Roy, Baijayanta. n.d. <em>All about Feature Scaling</em>. <a href="https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35">https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35</a>.
</div>
<div id="ref-videnskabsteori" class="csl-entry" role="doc-biblioentry">
Sørensen, Henrik Kragh, and Mikkel Willum Johansen. 2020. <em>"Invitation Til de Datalogiske Fags Videnskabsteori". Lærebog Til Brug for Undervisning Ved Institut for Naturfagenes Didaktik, Københavns Universitet</em>. Under udarbejdelse.
</div>
</div>
</section>


</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>